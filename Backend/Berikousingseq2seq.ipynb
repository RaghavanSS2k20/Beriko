{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzPYVjH8_HTF",
        "outputId": "819528f2-d67a-4c5a-ff06-e45dd8d5bee0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "sw = stopwords.words('english')\n",
        "sw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3IPZNdlX5qJe"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "string.punctuation\n",
        "#defining the function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
        "    return punctuationfree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uPQ2q8M_irF",
        "outputId": "6c32191a-6614-43d3-d91c-9368b5fded89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['solution', 'question']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "#Stop words present in the library\n",
        "\n",
        "en_stopwords = stopwords.words('english')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    result = []\n",
        "    for token in text:\n",
        "        if token not in en_stopwords:\n",
        "            result.append(token)\n",
        "            \n",
        "    return result\n",
        "\n",
        "#Test\n",
        "text = \"this is the only solution of that question\".split() \n",
        "remove_stopwords(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\Raghavan\n",
            "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32md:\\Beriko Theapp\\Beriko\\Backend\\Berikousingseq2seq.ipynb Cell 6\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Beriko%20Theapp/Beriko/Backend/Berikousingseq2seq.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Beriko%20Theapp/Beriko/Backend/Berikousingseq2seq.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m nltk\u001b[39m.\u001b[39;49mdownload()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\downloader.py:763\u001b[0m, in \u001b[0;36mDownloader.download\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[39mif\u001b[39;00m download_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_dir \u001b[39m=\u001b[39m download_dir\n\u001b[1;32m--> 763\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interactive_download()\n\u001b[0;32m    764\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    767\u001b[0m     \u001b[39m# Define a helper function for displaying output:\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\downloader.py:1113\u001b[0m, in \u001b[0;36mDownloader._interactive_download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1111\u001b[0m \u001b[39mif\u001b[39;00m TKINTER:\n\u001b[0;32m   1112\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1113\u001b[0m         DownloaderGUI(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mmainloop()\n\u001b[0;32m   1114\u001b[0m     \u001b[39mexcept\u001b[39;00m TclError:\n\u001b[0;32m   1115\u001b[0m         DownloaderShell(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mrun()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\downloader.py:1932\u001b[0m, in \u001b[0;36mDownloaderGUI.mainloop\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1931\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmainloop\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 1932\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtop\u001b[39m.\u001b[39mmainloop(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.1776.0_x64__qbz5n2kfra8p0\\lib\\tkinter\\__init__.py:1429\u001b[0m, in \u001b[0;36mMisc.mainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmainloop\u001b[39m(\u001b[39mself\u001b[39m, n\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m   1428\u001b[0m     \u001b[39m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1429\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtk\u001b[39m.\u001b[39;49mmainloop(n)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('tokeni)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "def tokenization(text):\n",
        "    tokens = re.split('W+',text)\n",
        "    return tokens\n",
        "#applying function to the column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "plsSsTqX3oWP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Show Number    Air Date      Round                         Category  Value  \\\n",
            "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
            "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
            "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
            "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
            "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
            "\n",
            "                                            Question      Answer  \n",
            "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
            "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
            "2  The city of Yuma in this state has a record av...     Arizona  \n",
            "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
            "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  \n"
          ]
        }
      ],
      "source": [
        "jeopardy_dataframe = pd.read_csv(\"dataset\\JEOPARDY_CSV.csv\")\n",
        "print(jeopardy_dataframe.head())\n",
        "questions = jeopardy_dataframe[' Question']\n",
        "answers = jeopardy_dataframe[' Answer']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "jeopardy_dataframe[' Cleaned_Question'] = jeopardy_dataframe[' Question'].apply(lambda x:remove_punctuation(x))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Show Number</th>\n",
              "      <th>Air Date</th>\n",
              "      <th>Round</th>\n",
              "      <th>Category</th>\n",
              "      <th>Value</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Cleaned_Question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4680</td>\n",
              "      <td>2004-12-31</td>\n",
              "      <td>Jeopardy!</td>\n",
              "      <td>HISTORY</td>\n",
              "      <td>$200</td>\n",
              "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
              "      <td>Copernicus</td>\n",
              "      <td>[For the last 8 years of his life Galileo was ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4680</td>\n",
              "      <td>2004-12-31</td>\n",
              "      <td>Jeopardy!</td>\n",
              "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
              "      <td>$200</td>\n",
              "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
              "      <td>Jim Thorpe</td>\n",
              "      <td>[No 2 1912 Olympian football star at Carlisle ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4680</td>\n",
              "      <td>2004-12-31</td>\n",
              "      <td>Jeopardy!</td>\n",
              "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
              "      <td>$200</td>\n",
              "      <td>The city of Yuma in this state has a record av...</td>\n",
              "      <td>Arizona</td>\n",
              "      <td>[The city of Yuma in this state has a record a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4680</td>\n",
              "      <td>2004-12-31</td>\n",
              "      <td>Jeopardy!</td>\n",
              "      <td>THE COMPANY LINE</td>\n",
              "      <td>$200</td>\n",
              "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
              "      <td>McDonald's</td>\n",
              "      <td>[In 1963 live on The Art Linkletter Show this ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4680</td>\n",
              "      <td>2004-12-31</td>\n",
              "      <td>Jeopardy!</td>\n",
              "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
              "      <td>$200</td>\n",
              "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
              "      <td>John Adams</td>\n",
              "      <td>[Signer of the Dec of Indep framer of the Cons...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Show Number    Air Date      Round                         Category  Value  \\\n",
              "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
              "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
              "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
              "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
              "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
              "\n",
              "                                            Question      Answer  \\\n",
              "0  For the last 8 years of his life, Galileo was ...  Copernicus   \n",
              "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe   \n",
              "2  The city of Yuma in this state has a record av...     Arizona   \n",
              "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's   \n",
              "4  Signer of the Dec. of Indep., framer of the Co...  John Adams   \n",
              "\n",
              "                                    Cleaned_Question  \n",
              "0  [For the last 8 years of his life Galileo was ...  \n",
              "1  [No 2 1912 Olympian football star at Carlisle ...  \n",
              "2  [The city of Yuma in this state has a record a...  \n",
              "3  [In 1963 live on The Art Linkletter Show this ...  \n",
              "4  [Signer of the Dec of Indep framer of the Cons...  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "jeopardy_dataframe[\" Cleaned_Question\"] = jeopardy_dataframe[' Cleaned_Question'].apply(tokenization)\n",
        "jeopardy_dataframe[' Cleaned_Question'] = jeopardy_dataframe[' Cleaned_Question'].apply(remove_stopwords)\n",
        "jeopardy_dataframe.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatizer(text):\n",
        "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
        "    return lemm_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "jeopardy_dataframe[' Cleaned_Question'] = jeopardy_dataframe[' Cleaned_Question'].apply(lemmatizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "questions = jeopardy_dataframe[' Cleaned_Question']\n",
        "tokenizer.fit_on_texts(questions)\n",
        "sequences = tokenizer.texts_to_sequences(questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_len = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_size = int(len(padded_sequences) * 0.8)\n",
        "train_data = padded_sequences[:train_size]\n",
        "val_data = padded_sequences[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_generator(encoder_input_data, decoder_input_data, batch_size, num_classes):\n",
        "    num_batches = len(encoder_input_data) // batch_size\n",
        "    while True:\n",
        "        for i in range(num_batches):\n",
        "            encoder_batch = encoder_input_data[i * batch_size:(i + 1) * batch_size]\n",
        "            decoder_batch = decoder_input_data[i * batch_size:(i + 1) * batch_size]\n",
        "            decoder_target_batch = to_categorical(decoder_batch, num_classes=num_classes)\n",
        "            yield ([encoder_batch, decoder_batch], decoder_target_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder_input_data = train_data[:, :-1]\n",
        "decoder_input_data = train_data[:, 1:]\n",
        "train_gen = data_generator(encoder_input_data, decoder_input_data, batch_size=64, num_classes=5000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from keras.losses import sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "`validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'generator'>]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32md:\\Beriko Theapp\\Beriko\\Backend\\Berikousingseq2seq.ipynb Cell 19\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Beriko%20Theapp/Beriko/Backend/Berikousingseq2seq.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model \u001b[39m=\u001b[39m Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Beriko%20Theapp/Beriko/Backend/Berikousingseq2seq.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Beriko%20Theapp/Beriko/Backend/Berikousingseq2seq.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_gen, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(encoder_input_data) \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m64\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\data_adapter.py:1480\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[1;34m(arrays, validation_split)\u001b[0m\n\u001b[0;32m   1478\u001b[0m unsplitable \u001b[39m=\u001b[39m [\u001b[39mtype\u001b[39m(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m flat_arrays \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _can_split(t)]\n\u001b[0;32m   1479\u001b[0m \u001b[39mif\u001b[39;00m unsplitable:\n\u001b[1;32m-> 1480\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1481\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m`validation_split` is only supported for Tensors or NumPy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1482\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39marrays, found following types in the input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(unsplitable))\n\u001b[0;32m   1484\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(t \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m flat_arrays):\n\u001b[0;32m   1485\u001b[0m   \u001b[39mreturn\u001b[39;00m arrays, arrays\n",
            "\u001b[1;31mValueError\u001b[0m: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'generator'>]"
          ]
        }
      ],
      "source": [
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(input_dim=5000, output_dim=128)(encoder_inputs)\n",
        "encoder_lstm = LSTM(units=128, return_state=True)\n",
        "_, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(input_dim=5000, output_dim=128)(decoder_inputs)\n",
        "decoder_lstm = LSTM(units=128, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(units=5000, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_gen, epochs=50, steps_per_epoch=len(encoder_input_data) // 64, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Raghavan\n",
            "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
